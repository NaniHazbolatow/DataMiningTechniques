{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Assuming train_data_with_agg and test_data_with_agg are already prepared with aggregated features\n",
    "# from the previous code snippet\n",
    "\n",
    "# loading in the cleaned training data\n",
    "train_data = pl.read_csv('data/cleaned_training_data.csv')\n",
    "test_data = pl.read_csv('data/cleaned_test_data.csv')\n",
    "\n",
    "# I will drop unnecessary columns\n",
    "train_data = train_data.drop(['prop_log_historical_price', 'price_usd', 'parsed_date', 'year', 'month', 'day', 'search_hour', 'day_of_week', 'year_month', 'date_time', 'prop_historical_price', 'price_usd_per_night_test', 'price_ratio', 'position', 'gross_bookings_usd', 'click_bool']). \\\n",
    "    rename({'price_usd_without_promo': 'price_usd'})\n",
    "\n",
    "test_data = test_data.drop(['prop_log_historical_price', 'price_usd', 'parsed_date', 'year', 'month', 'day', 'search_hour', 'day_of_week', 'year_month', 'date_time', 'prop_historical_price', 'price_usd_per_night_test', 'price_ratio']). \\\n",
    "    rename({'price_usd_without_promo': 'price_usd'})\n",
    "\n",
    "# Convert to pandas for easier manipulation with scikit-learn\n",
    "train_df = train_data.to_pandas()\n",
    "test_df = test_data.to_pandas()\n",
    "\n",
    "# Extract features, groups, and labels\n",
    "all_feature_cols = [col for col in train_df.columns if col not in \n",
    "                    ['srch_id', 'prop_id', 'booking_bool', 'click_bool', 'gross_bookings_usd']]\n",
    "\n",
    "X_train = train_df[all_feature_cols].values\n",
    "y_train = train_df['booking_bool'].values\n",
    "groups_train = train_df['srch_id'].values\n",
    "\n",
    "X_test = test_df[all_feature_cols].values\n",
    "groups_test = test_df['srch_id'].values\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define evaluation function for NDCG\n",
    "def calculate_ndcg(y_true, y_pred, group_indices, k=5):\n",
    "    \"\"\"Calculate NDCG@k for a set of queries.\"\"\"\n",
    "    ndcg_scores = []\n",
    "    \n",
    "    # Process each group (query)\n",
    "    start_idx = 0\n",
    "    for group_size in group_indices:\n",
    "        if group_size <= 1:  # Skip queries with only one result\n",
    "            start_idx += group_size\n",
    "            continue\n",
    "            \n",
    "        end_idx = start_idx + group_size\n",
    "        \n",
    "        # Get true relevance and predictions for this query\n",
    "        query_y_true = y_true[start_idx:end_idx]\n",
    "        query_y_pred = y_pred[start_idx:end_idx]\n",
    "        \n",
    "        # Calculate NDCG for this query\n",
    "        # If all relevance scores are 0, skip this query\n",
    "        if np.sum(query_y_true) > 0:\n",
    "            # Limit k to the size of the group\n",
    "            actual_k = min(k, len(query_y_true))\n",
    "            \n",
    "            # Compute NDCG\n",
    "            try:\n",
    "                score = ndcg_score(\n",
    "                    np.asarray([query_y_true]), \n",
    "                    np.asarray([query_y_pred]), \n",
    "                    k=actual_k\n",
    "                )\n",
    "                ndcg_scores.append(score)\n",
    "            except:\n",
    "                # Handle any errors in NDCG calculation\n",
    "                pass\n",
    "        \n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # Return average NDCG across all queries\n",
    "    if len(ndcg_scores) > 0:\n",
    "        return np.mean(ndcg_scores)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Define objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define parameters to tune\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'boosting_type': 'gbdt',\n",
    "        # Key parameters to tune\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 255),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        # LambdaMART specific parameters\n",
    "        'max_position': trial.suggest_int('max_position', 5, 20),\n",
    "        # Other parameters\n",
    "        'verbosity': -1,\n",
    "        'ndcg_eval_at': [5]\n",
    "    }\n",
    "    \n",
    "    # Set up GroupKFold for cross-validation\n",
    "    n_folds = 3\n",
    "    group_kfold = GroupKFold(n_splits=n_folds)\n",
    "    \n",
    "    # Store CV scores\n",
    "    cv_scores = []\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    for train_idx, valid_idx in group_kfold.split(X_train, y_train, groups=groups_train):\n",
    "        X_train_fold, X_valid_fold = X_train[train_idx], X_train[valid_idx]\n",
    "        y_train_fold, y_valid_fold = y_train[train_idx], y_train[valid_idx]\n",
    "        \n",
    "        # Get group information for training and validation sets\n",
    "        train_groups = np.array(pd.Series(groups_train[train_idx]).groupby(groups_train[train_idx]).count())\n",
    "        valid_groups = np.array(pd.Series(groups_train[valid_idx]).groupby(groups_train[valid_idx]).count())\n",
    "        \n",
    "        # Create LightGBM datasets\n",
    "        train_dataset = lgb.Dataset(X_train_fold, y_train_fold, group=train_groups)\n",
    "        valid_dataset = lgb.Dataset(X_valid_fold, y_valid_fold, group=valid_groups, reference=train_dataset)\n",
    "        \n",
    "        # Train model\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_dataset,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[valid_dataset],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Predict on validation set\n",
    "        valid_preds = model.predict(X_valid_fold, num_iteration=model.best_iteration)\n",
    "        \n",
    "        # Calculate NDCG score\n",
    "        ndcg_score = calculate_ndcg(y_valid_fold, valid_preds, valid_groups, k=5)\n",
    "        cv_scores.append(ndcg_score)\n",
    "    \n",
    "    # Return mean NDCG score across folds\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Create Optuna study for hyperparameter optimization\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # Adjust number of trials based on your computational resources\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters found:\")\n",
    "print(study.best_params)\n",
    "print(f\"Best NDCG score: {study.best_value}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'ndcg_eval_at': [5]\n",
    "})\n",
    "\n",
    "# Get group information for full training set\n",
    "train_groups = np.array(pd.Series(groups_train).groupby(groups_train).count())\n",
    "test_groups = np.array(pd.Series(groups_test).groupby(groups_test).count())\n",
    "\n",
    "# Create final training dataset\n",
    "final_train_dataset = lgb.Dataset(X_train, y_train, group=train_groups)\n",
    "\n",
    "# Train final model\n",
    "print(\"Training final model with best parameters...\")\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    final_train_dataset,\n",
    "    num_boost_round=2000  # Increase num_boost_round for final model\n",
    ")\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"Making predictions on test set...\")\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "# Add predictions to test dataframe\n",
    "test_df['predictions'] = test_predictions\n",
    "\n",
    "# Create submission file\n",
    "result_df = test_df[['srch_id', 'prop_id', 'predictions']]\n",
    "result_df = result_df.sort_values(['srch_id', 'predictions'], ascending=[True, False])\n",
    "result_df.to_csv('lambdamart_optimized_predictions.csv', index=False)\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = final_model.feature_importance(importance_type='gain')\n",
    "feature_names = all_feature_cols\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "print(\"Top 20 important features:\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Additional analysis: NDCG score on test set\n",
    "# Note: Since we don't have true relevance for test data, this is only applicable\n",
    "# if your test data actually contains relevance judgments\n",
    "if 'booking_bool' in test_df.columns:\n",
    "    y_test = test_df['booking_bool'].values\n",
    "    test_ndcg = calculate_ndcg(y_test, test_predictions, test_groups, k=5)\n",
    "    print(f\"NDCG@10 on test set: {test_ndcg}\")\n",
    "\n",
    "# Save the model for future use\n",
    "final_model.save_model('lambdamart_optimized_model.txt')\n",
    "print(\"Hyperparameter optimization and model training completed!\")\n",
    "\n",
    "# Create detailed parameter importance visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna.visualization as optvis\n",
    "\n",
    "# Parameter importance plot\n",
    "try:\n",
    "    param_importance = optvis.plot_param_importances(study)\n",
    "    param_importance.write_image('parameter_importance.png')\n",
    "    \n",
    "    # Optimization history\n",
    "    optimization_history = optvis.plot_optimization_history(study)\n",
    "    optimization_history.write_image('optimization_history.png')\n",
    "except:\n",
    "    print(\"Could not create visualization - may need plotly installed\")\n",
    "\n",
    "# Create a visualization for feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(importance_df['Feature'].head(30), importance_df['Importance'].head(30))\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 30 Features by Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f5977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the srch_id and prop_id for the full predictions\n",
    "submission_df = test_df[['srch_id', 'prop_id']]\n",
    "submission_df = submission_df.sort_values(['srch_id'], ascending=[True])\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
