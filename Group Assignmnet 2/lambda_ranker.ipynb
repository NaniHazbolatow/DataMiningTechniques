{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e981795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "# loading in the cleaned training data\n",
    "train_data = pl.read_csv('data/cleaned_training_data.csv')\n",
    "test_data = pl.read_csv('data/cleaned_test_data.csv')\n",
    "\n",
    "# Print all features except ['prop_log_historical_price', 'price_usd', 'parsed_date', 'year', 'month', 'day', 'search_hour', 'day_of_week', 'year_month', 'date_time', 'prop_historical_price', 'price_usd_per_night_test', 'price_ratio', 'position', 'gross_bookings_usd', 'click_bool']\n",
    "# excluded_features = ['prop_log_historical_price', 'price_usd', 'parsed_date', 'year', 'month', 'day', 'search_hour', 'day_of_week', 'year_month', 'date_time', 'prop_historical_price', 'price_usd_per_night_test', 'price_ratio', 'position', 'gross_bookings_usd', 'click_bool', 'random_bool', 'site_id', 'promotion_flag', 'is_weekend_search', 'month_sin', 'search_hour_sin', 'day_of_week_sin', 'has_usable_review', 'query_affinity_missing']\n",
    "# features = [col for col in train_data.columns if col not in excluded_features]\n",
    "# print(\"Features used for training:\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ea8c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data columns: ['srch_id', 'visitor_location_country_id', 'visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_country_id', 'prop_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window', 'srch_adults_count', 'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool', 'srch_query_affinity_score', 'orig_destination_distance', 'gross_bookings_usd', 'booking_bool', 'month_cos', 'search_hour_cos', 'day_of_week_cos', 'prop_review_score_filled', 'num_comps_lower', 'num_comps_higher', 'num_comps_with_inventory', 'avg_comp_rate_percent_diff', 'query_affinity_score_cleaned', 'price_usd']\n",
      "Training data schema: Schema({'srch_id': Int64, 'visitor_location_country_id': Int64, 'visitor_hist_starrating': String, 'visitor_hist_adr_usd': String, 'prop_country_id': Int64, 'prop_id': Int64, 'prop_starrating': Int64, 'prop_review_score': Float64, 'prop_brand_bool': Int64, 'prop_location_score1': Float64, 'prop_location_score2': Float64, 'srch_destination_id': Int64, 'srch_length_of_stay': Int64, 'srch_booking_window': Int64, 'srch_adults_count': Int64, 'srch_children_count': Int64, 'srch_room_count': Int64, 'srch_saturday_night_bool': Int64, 'srch_query_affinity_score': String, 'orig_destination_distance': Float64, 'gross_bookings_usd': Float64, 'booking_bool': Int64, 'month_cos': Float64, 'search_hour_cos': Float64, 'day_of_week_cos': Float64, 'prop_review_score_filled': Float64, 'num_comps_lower': Int64, 'num_comps_higher': Int64, 'num_comps_with_inventory': Int64, 'avg_comp_rate_percent_diff': Float64, 'query_affinity_score_cleaned': Float64, 'price_usd': Float64})\n",
      "Identified 25 numeric features: ['visitor_location_country_id', 'prop_country_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window', 'srch_adults_count', 'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool', 'orig_destination_distance', 'month_cos', 'search_hour_cos', 'day_of_week_cos', 'prop_review_score_filled', 'num_comps_lower', 'num_comps_higher', 'num_comps_with_inventory', 'avg_comp_rate_percent_diff', 'query_affinity_score_cleaned', 'price_usd']\n",
      "Training LambdaMART model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UvA\\Semester II\\Period II\\Data Mining Techniques\\DataMiningTechniques\\.venv\\Lib\\site-packages\\lightgbm\\callback.py:347: UserWarning: Only training set found, disabling early stopping.\n",
      "  _log_warning(\"Only training set found, disabling early stopping.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's ndcg@5: 0.592308\n",
      "[200]\ttraining's ndcg@5: 0.601624\n",
      "[300]\ttraining's ndcg@5: 0.609171\n",
      "[400]\ttraining's ndcg@5: 0.615976\n",
      "[500]\ttraining's ndcg@5: 0.621843\n",
      "[600]\ttraining's ndcg@5: 0.627757\n",
      "[700]\ttraining's ndcg@5: 0.632732\n",
      "[800]\ttraining's ndcg@5: 0.637915\n",
      "[900]\ttraining's ndcg@5: 0.642621\n",
      "[1000]\ttraining's ndcg@5: 0.6473\n",
      "Making predictions...\n",
      "Top 20 important features:\n",
      "                                       Feature     Importance\n",
      "6                         prop_location_score2  877222.434440\n",
      "24                                   price_usd  401124.861244\n",
      "99                   price_usd_median_per_prop  142655.361441\n",
      "97                     price_usd_mean_per_prop  131263.596019\n",
      "40          prop_location_score1_mean_per_prop  114447.633153\n",
      "43          prop_location_score2_mean_per_prop   88993.662494\n",
      "2                              prop_starrating   86101.765804\n",
      "5                         prop_location_score1   84214.518615\n",
      "45        prop_location_score2_median_per_prop   72006.330319\n",
      "3                            prop_review_score   60566.972991\n",
      "44        prop_location_score2_stddev_per_prop   58455.111636\n",
      "98                   price_usd_stddev_per_prop   53162.656455\n",
      "49           srch_length_of_stay_mean_per_prop   48690.453966\n",
      "23                query_affinity_score_cleaned   47853.985635\n",
      "64      srch_saturday_night_bool_mean_per_prop   43287.888237\n",
      "62             srch_room_count_stddev_per_prop   43272.231616\n",
      "31               prop_starrating_mean_per_prop   43257.827236\n",
      "9                          srch_booking_window   37401.406631\n",
      "92  avg_comp_rate_percent_diff_stddev_per_prop   37137.574404\n",
      "83             num_comps_lower_stddev_per_prop   34302.662542\n",
      "LambdaMART model training completed!\n"
     ]
    }
   ],
   "source": [
    "# I will drop unnecessary columns, but only those that exist in the DataFrame\n",
    "drop_cols = [\n",
    "    'prop_log_historical_price', 'price_usd', 'parsed_date', 'year', 'month', 'day', 'search_hour',\n",
    "    'day_of_week', 'year_month', 'date_time', 'prop_historical_price', 'price_usd_per_night_test',\n",
    "    'price_ratio', 'position', 'click_bool', 'random_bool', 'site_id', 'promotion_flag',\n",
    "    'is_weekend_search', 'month_sin', 'search_hour_sin', 'day_of_week_sin', 'has_usable_review',\n",
    "    'query_affinity_missing'\n",
    "]\n",
    "\n",
    "train_data = train_data.drop([col for col in drop_cols if col in train_data.columns]).rename({'price_usd_without_promo': 'price_usd'} if 'price_usd_without_promo' in train_data.columns else {})\n",
    "\n",
    "test_data = test_data.drop([col for col in drop_cols if col in test_data.columns]).rename({'price_usd_without_promo': 'price_usd'} if 'price_usd_without_promo' in test_data.columns else {})\n",
    "\n",
    "# Convert to pandas for easier manipulation with scikit-learn\n",
    "train_df = train_data.to_pandas()\n",
    "test_df = test_data.to_pandas()\n",
    "# 1. Identify numeric features\n",
    "# We'll exclude categorical features and target variables\n",
    "# Let's first check the data types and columns\n",
    "print(\"Training data columns:\", train_data.columns)\n",
    "print(\"Training data schema:\", train_data.schema)\n",
    "\n",
    "# Let's assume we'll identify numeric features based on their data types\n",
    "numeric_cols = [col for col in train_data.columns if \n",
    "                train_data[col].dtype in [pl.Float32, pl.Float64, pl.Int32, pl.Int64] and \n",
    "                col not in ['prop_id', 'srch_id', 'booking_bool', 'click_bool', 'gross_bookings_usd']]\n",
    "\n",
    "print(f\"Identified {len(numeric_cols)} numeric features: {numeric_cols}\")\n",
    "\n",
    "# 2. Calculate aggregate features per prop_id\n",
    "# For each numeric feature, we'll calculate average, stddev, and median per prop_id\n",
    "\n",
    "# First, let's create functions to calculate these aggregates\n",
    "def add_aggregated_features(data, numeric_cols):\n",
    "    # Create copies to avoid modifying original\n",
    "    result_df = data.clone()\n",
    "    \n",
    "    # Calculate aggregates for each numeric column\n",
    "    for col in numeric_cols:\n",
    "        # Calculate mean per prop_id\n",
    "        mean_per_prop = data.group_by('prop_id').agg(pl.mean(col).alias(f\"{col}_mean_per_prop\"))\n",
    "        result_df = result_df.join(mean_per_prop, on='prop_id')\n",
    "        \n",
    "        # Calculate stddev per prop_id\n",
    "        stddev_per_prop = data.group_by('prop_id').agg(pl.std(col).alias(f\"{col}_stddev_per_prop\"))\n",
    "        result_df = result_df.join(stddev_per_prop, on='prop_id')\n",
    "        \n",
    "        # Calculate median per prop_id\n",
    "        median_per_prop = data.group_by('prop_id').agg(pl.median(col).alias(f\"{col}_median_per_prop\"))\n",
    "        result_df = result_df.join(median_per_prop, on='prop_id')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Add aggregated features to both train and test sets\n",
    "train_data_with_agg = add_aggregated_features(train_data, numeric_cols)\n",
    "test_data_with_agg = add_aggregated_features(test_data, numeric_cols)\n",
    "\n",
    "# 3. Prepare data for LambdaMART (LightGBM)\n",
    "# For LightGBM ranking task, we need to prepare data in the correct format\n",
    "# We need features, groups (queries/srch_id), and labels (relevance)\n",
    "\n",
    "# Get all feature columns (original numeric + aggregated)\n",
    "all_feature_cols = numeric_cols.copy()\n",
    "for col in numeric_cols:\n",
    "    all_feature_cols.extend([f\"{col}_mean_per_prop\", f\"{col}_stddev_per_prop\", f\"{col}_median_per_prop\"])\n",
    "\n",
    "# Replace NaN values with 0\n",
    "train_data_with_agg = train_data_with_agg.fill_null(0)\n",
    "test_data_with_agg = test_data_with_agg.fill_null(0)\n",
    "\n",
    "# Convert to pandas for easier use with LightGBM\n",
    "train_df = train_data_with_agg.to_pandas()\n",
    "test_df = test_data_with_agg.to_pandas()\n",
    "\n",
    "# Extract features, groups, and labels\n",
    "X_train = train_df[all_feature_cols].values\n",
    "# For ranking, we'll use booking_bool as the relevance label\n",
    "y_train = train_df['booking_bool'].values\n",
    "# Group by srch_id for ranking\n",
    "groups_train = train_df.groupby('srch_id').size().values\n",
    "\n",
    "X_test = test_df[all_feature_cols].values\n",
    "groups_test = test_df.groupby('srch_id').size().values\n",
    "\n",
    "# Optional: Scale features for better performance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 4. Train LambdaMART model using LightGBM\n",
    "# Create LightGBM dataset\n",
    "train_dataset = lgb.Dataset(X_train, y_train, group=groups_train)\n",
    "\n",
    "# Set parameters for LambdaMART\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'min_data_in_leaf': 57,\n",
    "    'num_leaves': 76,\n",
    "    'learning_rate': 0.024720389075475732,\n",
    "    'feature_fraction': 0.6572262837001418,\n",
    "    'bagging_fraction': 0.8834318020487413,\n",
    "    'bagging_freq': 10,\n",
    "    'lambda_l1': 3.9616918691847163,\n",
    "    'lambda_l2': 0.0016270516531082348,\n",
    "    'max_position': 12,  # Unlimited position\n",
    "    'max_depth': 12,  # Unlimited depth\n",
    "    'verbosity': -1,\n",
    "    'ndcg_eval_at': [5],  # Evaluate NDCG at position 5\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "print(\"Training LambdaMART model...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_dataset,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[train_dataset],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50, verbose=True),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 5. Make predictions and evaluate\n",
    "print(\"Making predictions...\")\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Save the predictions to file for further analysis or submission\n",
    "test_df['predictions'] = test_predictions\n",
    "result_df = test_df[['srch_id', 'prop_id', 'predictions']]\n",
    "\n",
    "# Sort by srch_id and prediction score (descending) to get the final ranking\n",
    "result_df = result_df.sort_values(['srch_id', 'predictions'], ascending=[True, False])\n",
    "result_df.to_csv('lambdamart_predictions.csv', index=False)\n",
    "\n",
    "# 6. Feature importance analysis\n",
    "feature_importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = all_feature_cols\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "print(\"Top 20 important features:\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "print(\"LambdaMART model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43228ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a7f5977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the srch_id and prop_id for the full predictions\n",
    "submission_df = test_df[['srch_id', 'prop_id']]\n",
    "submission_df = submission_df.sort_values(['srch_id'], ascending=[True])\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
