{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:33:57.065609Z",
     "start_time": "2025-05-15T19:33:40.482969Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRanker, Pool\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# loading in the cleaned training data\n",
    "train_data = pl.read_csv('data/cleaned_training_data.csv')\n",
    "test_data = pl.read_csv('data/cleaned_test_data.csv')\n",
    "\n",
    "# I will drop unnecessary columns\n",
    "train_data = train_data.drop(['prop_log_historical_price', 'price_usd', 'parsed_date', 'year', 'month', 'day', 'search_hour', 'day_of_week', 'year_month', 'date_time', 'prop_historical_price', 'price_usd_per_night_test', 'price_ratio', 'position', 'gross_bookings_usd', 'click_bool']). \\\n",
    "    rename({'price_usd_without_promo': 'price_usd'})\n",
    "\n",
    "test_data = test_data.drop(['prop_log_historical_price', 'price_usd', 'parsed_date', 'year', 'month', 'day', 'search_hour', 'day_of_week', 'year_month', 'date_time', 'prop_historical_price', 'price_usd_per_night_test', 'price_ratio']). \\\n",
    "    rename({'price_usd_without_promo': 'price_usd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b7674f2f49a4a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T19:15:03.926879Z",
     "start_time": "2025-05-15T19:14:57.827589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1243975, 40)\n"
     ]
    }
   ],
   "source": [
    "# creating the dataframe for the CatBoostRanker\n",
    "cbr_data = train_data.to_pandas().copy()\n",
    "\n",
    "# I will train the model on a subset\n",
    "cbr_data = cbr_data[cbr_data['srch_id'].isin(cbr_data['srch_id'].unique()[:50000])]\n",
    "print(cbr_data.shape) # 2492 observations\n",
    "\n",
    "# test model subset\n",
    "test_subset = test_data.to_pandas().copy()\n",
    "test_subset = test_subset[test_subset['srch_id'].isin(cbr_data['srch_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89501a48e4b192d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T20:00:48.924546Z",
     "start_time": "2025-05-15T20:00:48.299243Z"
    }
   },
   "outputs": [],
   "source": [
    "# I will set up a CatBoostRanker. It handles numeric variables natively, so only the categorical ones need to be specified (it assumes everything else is numeric).\n",
    "\n",
    "# this column is used as a sort of relevance feature for the ranking, as the most relevant hotels are the ones the customers actually booked. This will help the model decide what is most important\n",
    "cbr_data['target'] = cbr_data['booking_bool']\n",
    "\n",
    "# now we exclude target-only columns\n",
    "exclude_cols = ['target', 'booking_bool', 'srch_id', 'prop_id']\n",
    "feature_cols = [col for col in cbr_data.columns if col not in exclude_cols]\n",
    "\n",
    "# specifying categorical features\n",
    "cat_features = [col for col in cbr_data[feature_cols].columns if cbr_data[col].dtype == 'object' or 'id' in col]\n",
    "\n",
    "# do the train/test split by group (srch_id). This is to prevent the data being split such that both the training and test sets contain the same search id\n",
    "unique_srch_ids = cbr_data['srch_id'].unique()\n",
    "train_srch, val_srch = train_test_split(unique_srch_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# actually creating the training and validation sets\n",
    "train_subset = cbr_data[cbr_data['srch_id'].isin(train_srch)].copy()\n",
    "val_subset = cbr_data[cbr_data['srch_id'].isin(val_srch)].copy()\n",
    "\n",
    "train_subset = train_subset.sort_values('srch_id')\n",
    "val_subset = val_subset.sort_values('srch_id')\n",
    "\n",
    "# impute missing numeric values\n",
    "for col in feature_cols:\n",
    "    if train_subset[col].dtype in [np.float64, np.int64]:\n",
    "        train_subset[col] = train_subset[col].fillna(-999)\n",
    "    if val_subset[col].dtype in [np.float64, np.int64]:\n",
    "        val_subset[col] = val_subset[col].fillna(-999)\n",
    "\n",
    "# same with categorical\n",
    "for col in cat_features:\n",
    "    train_subset[col] = train_subset[col].fillna('missing')\n",
    "    val_subset[col] = val_subset[col].fillna('missing')\n",
    "\n",
    "# Helper to compute group sizes\n",
    "def get_group_sizes(df, group_key='srch_id'):\n",
    "    return df.groupby(group_key).size().tolist()\n",
    "\n",
    "# Group sizes for CatBoost\n",
    "train_group_sizes = get_group_sizes(train_subset)\n",
    "val_group_sizes = get_group_sizes(val_subset)\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=train_subset[feature_cols],\n",
    "    label=train_subset['target'],\n",
    "    group_id=train_subset['srch_id'].astype(int).tolist(),\n",
    "    cat_features=cat_features\n",
    ")\n",
    "\n",
    "val_pool = Pool(\n",
    "    data=val_subset[feature_cols],\n",
    "    label=val_subset['target'],\n",
    "    group_id=val_subset['srch_id'].astype(int).tolist(),\n",
    "    cat_features=cat_features\n",
    ")\n",
    "\n",
    "# hyperparameter optimization function using optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"iterations\": 500,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 1e-9, 10),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0, 1),\n",
    "        \"loss_function\": \"YetiRank\",\n",
    "        \"random_seed\": 42,\n",
    "        \"verbose\": 0\n",
    "    }\n",
    "\n",
    "    model = CatBoostRanker(**params)\n",
    "    model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=30, verbose=0)\n",
    "\n",
    "    preds = model.predict(val_pool)\n",
    "\n",
    "    # Evaluate using NDCG@5\n",
    "    val_preds = val_subset.copy()\n",
    "    val_preds['pred'] = preds\n",
    "    val_preds = val_preds.sort_values(by=['srch_id', 'pred'], ascending=[True, False])\n",
    "\n",
    "    ndcg_scores = []\n",
    "    for srch_id, group in val_preds.groupby('srch_id'):\n",
    "        y_true = group['target'].values\n",
    "        y_score = group['pred'].values\n",
    "        if len(y_true) > 1:  # NDCG isn't defined for length 1\n",
    "            ndcg_scores.append(ndcg_score([y_true], [y_score], k=5))\n",
    "\n",
    "    return np.mean(ndcg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f570aae0afcd16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T20:18:50.533974Z",
     "start_time": "2025-05-15T20:01:03.529435Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 14:14:52,125] A new study created in memory with name: no-name-3680557c-bffb-4948-8de9-92cd061cefc9\n",
      "[I 2025-05-17 14:18:20,512] Trial 0 finished with value: 0.2683810374043468 and parameters: {'learning_rate': 0.12135972776624952, 'depth': 10, 'l2_leaf_reg': 3.3335617645086293, 'random_strength': 2.1678334242715267, 'bagging_temperature': 0.6263378646502356}. Best is trial 0 with value: 0.2683810374043468.\n",
      "[I 2025-05-17 14:21:06,784] Trial 1 finished with value: 0.2682413128102511 and parameters: {'learning_rate': 0.09949034094126227, 'depth': 8, 'l2_leaf_reg': 2.1698774043657916, 'random_strength': 3.8700635817340343, 'bagging_temperature': 0.5403071674999499}. Best is trial 0 with value: 0.2683810374043468.\n",
      "[I 2025-05-17 14:23:52,794] Trial 2 finished with value: 0.26713757766538043 and parameters: {'learning_rate': 0.1399389830243865, 'depth': 7, 'l2_leaf_reg': 9.284922557783096, 'random_strength': 2.339575686631948, 'bagging_temperature': 0.16293968981245965}. Best is trial 0 with value: 0.2683810374043468.\n",
      "[I 2025-05-17 14:26:42,472] Trial 3 finished with value: 0.267604202532375 and parameters: {'learning_rate': 0.11484474019882866, 'depth': 8, 'l2_leaf_reg': 5.865979661117804, 'random_strength': 8.99853414585745, 'bagging_temperature': 0.689689983844853}. Best is trial 0 with value: 0.2683810374043468.\n",
      "[I 2025-05-17 14:29:01,614] Trial 4 finished with value: 0.2687837883861827 and parameters: {'learning_rate': 0.16306422267728687, 'depth': 6, 'l2_leaf_reg': 5.477413774258645, 'random_strength': 6.864270320075351, 'bagging_temperature': 0.420035812878508}. Best is trial 4 with value: 0.2687837883861827.\n",
      "[I 2025-05-17 14:31:19,012] Trial 5 finished with value: 0.26714459742600005 and parameters: {'learning_rate': 0.1713181689380124, 'depth': 5, 'l2_leaf_reg': 8.890786936116292, 'random_strength': 3.2749174169515203, 'bagging_temperature': 0.5937692608235351}. Best is trial 4 with value: 0.2687837883861827.\n",
      "[I 2025-05-17 14:33:07,198] Trial 6 finished with value: 0.26890667485172864 and parameters: {'learning_rate': 0.14087933889366103, 'depth': 5, 'l2_leaf_reg': 4.004250181895592, 'random_strength': 9.730795564122813, 'bagging_temperature': 0.4433433737805209}. Best is trial 6 with value: 0.26890667485172864.\n",
      "[I 2025-05-17 14:35:52,951] Trial 7 finished with value: 0.26700031494078547 and parameters: {'learning_rate': 0.12239914711485639, 'depth': 5, 'l2_leaf_reg': 8.907931930651142, 'random_strength': 9.347180899880726, 'bagging_temperature': 0.02867921929861772}. Best is trial 6 with value: 0.26890667485172864.\n",
      "[I 2025-05-17 14:38:44,277] Trial 8 finished with value: 0.2672954211908698 and parameters: {'learning_rate': 0.17754221568735254, 'depth': 10, 'l2_leaf_reg': 2.455686132462582, 'random_strength': 3.2382431942563104, 'bagging_temperature': 0.7281098662373956}. Best is trial 6 with value: 0.26890667485172864.\n",
      "[I 2025-05-17 14:41:07,335] Trial 9 finished with value: 0.26636733557139464 and parameters: {'learning_rate': 0.1589542264220006, 'depth': 8, 'l2_leaf_reg': 8.875409428590507, 'random_strength': 5.957777546525155, 'bagging_temperature': 0.9877307633121115}. Best is trial 6 with value: 0.26890667485172864.\n",
      "[I 2025-05-17 14:46:31,037] Trial 10 finished with value: 0.2666241019151239 and parameters: {'learning_rate': 0.0482765588535389, 'depth': 4, 'l2_leaf_reg': 4.764306817863793, 'random_strength': 7.668907223236797, 'bagging_temperature': 0.32529876307346367}. Best is trial 6 with value: 0.26890667485172864.\n",
      "[I 2025-05-17 14:56:19,272] Trial 11 finished with value: 0.2666591692655706 and parameters: {'learning_rate': 0.079786723793532, 'depth': 6, 'l2_leaf_reg': 5.900083193513421, 'random_strength': 6.475155410172387, 'bagging_temperature': 0.3527542134518012}. Best is trial 6 with value: 0.26890667485172864.\n",
      "[I 2025-05-17 15:05:13,182] Trial 12 finished with value: 0.26839257345944845 and parameters: {'learning_rate': 0.18993235387522395, 'depth': 6, 'l2_leaf_reg': 4.360606163255045, 'random_strength': 9.98694161287249, 'bagging_temperature': 0.3621949332836739}. Best is trial 6 with value: 0.26890667485172864.\n",
      "[I 2025-05-17 15:17:28,998] Trial 13 finished with value: 0.2556892838039672 and parameters: {'learning_rate': 0.01485652533461787, 'depth': 4, 'l2_leaf_reg': 7.551725030481722, 'random_strength': 7.574951474600343, 'bagging_temperature': 0.4405919230360306}. Best is trial 6 with value: 0.26890667485172864.\n",
      "[I 2025-05-17 15:22:45,612] Trial 14 finished with value: 0.26958523891904185 and parameters: {'learning_rate': 0.1472854315500297, 'depth': 6, 'l2_leaf_reg': 1.0690358684850554, 'random_strength': 0.09148460986090967, 'bagging_temperature': 0.18647258138698108}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 15:30:32,513] Trial 15 finished with value: 0.26875071470742007 and parameters: {'learning_rate': 0.14349560939464975, 'depth': 5, 'l2_leaf_reg': 3.3238376420565636, 'random_strength': 0.9411958413489387, 'bagging_temperature': 0.20477271462867383}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 15:34:01,818] Trial 16 finished with value: 0.2684044934848733 and parameters: {'learning_rate': 0.0905628084906242, 'depth': 7, 'l2_leaf_reg': 1.1582357729564523, 'random_strength': 0.0756207913980635, 'bagging_temperature': 0.19586919919427448}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 15:37:54,977] Trial 17 finished with value: 0.2667056031649228 and parameters: {'learning_rate': 0.07315322506091615, 'depth': 4, 'l2_leaf_reg': 1.008217671232468, 'random_strength': 4.807606174367868, 'bagging_temperature': 0.01661108857961946}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 15:45:28,676] Trial 18 finished with value: 0.26680175982557736 and parameters: {'learning_rate': 0.14186217617866012, 'depth': 6, 'l2_leaf_reg': 7.1184162095701735, 'random_strength': 4.979794970963557, 'bagging_temperature': 0.9556542954261881}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 15:51:17,750] Trial 19 finished with value: 0.2679080362335492 and parameters: {'learning_rate': 0.19218342739853692, 'depth': 5, 'l2_leaf_reg': 2.3632997379095784, 'random_strength': 1.4022981302256423, 'bagging_temperature': 0.7805510992802343}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 16:05:29,109] Trial 20 finished with value: 0.26865246424409284 and parameters: {'learning_rate': 0.06046609214106288, 'depth': 9, 'l2_leaf_reg': 3.703590397498205, 'random_strength': 7.844562321738309, 'bagging_temperature': 0.27420462953263836}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 16:07:18,544] Trial 21 finished with value: 0.2680670418303408 and parameters: {'learning_rate': 0.15736618642181377, 'depth': 6, 'l2_leaf_reg': 5.1469936481638054, 'random_strength': 8.495451044652665, 'bagging_temperature': 0.4612880694035349}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 16:09:52,678] Trial 22 finished with value: 0.2671923031365207 and parameters: {'learning_rate': 0.1601749366742597, 'depth': 7, 'l2_leaf_reg': 6.391786363970214, 'random_strength': 6.575181570743252, 'bagging_temperature': 0.11390207663059415}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 16:12:17,932] Trial 23 finished with value: 0.26924577452744813 and parameters: {'learning_rate': 0.13394921410186372, 'depth': 6, 'l2_leaf_reg': 4.149841691371512, 'random_strength': 5.605014686998347, 'bagging_temperature': 0.4590723888310174}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 16:14:01,706] Trial 24 finished with value: 0.2669654885634387 and parameters: {'learning_rate': 0.13152396927823015, 'depth': 5, 'l2_leaf_reg': 4.028139964602181, 'random_strength': 5.654000107803476, 'bagging_temperature': 0.5132905399411862}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 16:16:32,406] Trial 25 finished with value: 0.26870010749684753 and parameters: {'learning_rate': 0.11154893194398909, 'depth': 7, 'l2_leaf_reg': 2.780463860739699, 'random_strength': 4.181430207101854, 'bagging_temperature': 0.26683298139015876}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 16:18:15,682] Trial 26 finished with value: 0.26932636259983495 and parameters: {'learning_rate': 0.14423722786419554, 'depth': 6, 'l2_leaf_reg': 1.7115971365522018, 'random_strength': 0.2811868017966148, 'bagging_temperature': 0.8336030626211998}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 16:20:32,085] Trial 27 finished with value: 0.26899580088955016 and parameters: {'learning_rate': 0.09909201139825094, 'depth': 6, 'l2_leaf_reg': 1.7375993230623483, 'random_strength': 0.8879225586799848, 'bagging_temperature': 0.8796345261937403}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 16:22:16,922] Trial 28 finished with value: 0.26903092444878624 and parameters: {'learning_rate': 0.17924021457491174, 'depth': 7, 'l2_leaf_reg': 1.223686318004766, 'random_strength': 2.2567305901679133, 'bagging_temperature': 0.8273838214082468}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 16:24:49,558] Trial 29 finished with value: 0.2667349403365508 and parameters: {'learning_rate': 0.1250706946001418, 'depth': 9, 'l2_leaf_reg': 3.2849878435856463, 'random_strength': 0.41979264019556034, 'bagging_temperature': 0.6300537863100822}. Best is trial 14 with value: 0.26958523891904185.\n",
      "[I 2025-05-17 16:27:09,464] Trial 30 finished with value: 0.2702772920654885 and parameters: {'learning_rate': 0.15016300399549445, 'depth': 6, 'l2_leaf_reg': 1.6703344196115146, 'random_strength': 1.4656659667834733, 'bagging_temperature': 0.6607812816531521}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 16:32:56,591] Trial 31 finished with value: 0.26909020457145394 and parameters: {'learning_rate': 0.15081989919939637, 'depth': 6, 'l2_leaf_reg': 1.7441143264751497, 'random_strength': 1.5851961534511299, 'bagging_temperature': 0.6713416965898157}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 16:37:54,099] Trial 32 finished with value: 0.2681610714292185 and parameters: {'learning_rate': 0.13540228665143683, 'depth': 7, 'l2_leaf_reg': 1.8396161492291978, 'random_strength': 0.02943506512786126, 'bagging_temperature': 0.5693318043538478}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 16:46:10,846] Trial 33 finished with value: 0.2688288667950519 and parameters: {'learning_rate': 0.1498562773665745, 'depth': 6, 'l2_leaf_reg': 2.849424283695094, 'random_strength': 1.6927248900832434, 'bagging_temperature': 0.8903756754798313}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 16:53:25,266] Trial 34 finished with value: 0.26905831102551125 and parameters: {'learning_rate': 0.12899495691444976, 'depth': 8, 'l2_leaf_reg': 1.9053313940983672, 'random_strength': 0.8013211867754393, 'bagging_temperature': 0.7489510373549704}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 16:58:50,868] Trial 35 finished with value: 0.2684105536252471 and parameters: {'learning_rate': 0.10873548454437942, 'depth': 6, 'l2_leaf_reg': 2.8094587501429507, 'random_strength': 2.7782883383119428, 'bagging_temperature': 0.6323919017873276}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:00:36,133] Trial 36 finished with value: 0.2674849774873338 and parameters: {'learning_rate': 0.11789198566023722, 'depth': 5, 'l2_leaf_reg': 1.4883814498311945, 'random_strength': 4.348615370371337, 'bagging_temperature': 0.8026892901809379}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:02:34,565] Trial 37 finished with value: 0.2697445732257711 and parameters: {'learning_rate': 0.16732107618062783, 'depth': 7, 'l2_leaf_reg': 2.305441999635519, 'random_strength': 1.9324662546795255, 'bagging_temperature': 0.5406345268430521}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:04:24,276] Trial 38 finished with value: 0.26883956414029664 and parameters: {'learning_rate': 0.17166783065354263, 'depth': 7, 'l2_leaf_reg': 2.282141795949252, 'random_strength': 2.042684872974329, 'bagging_temperature': 0.5548712597053296}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:06:21,202] Trial 39 finished with value: 0.2675156293656155 and parameters: {'learning_rate': 0.1825318847654001, 'depth': 8, 'l2_leaf_reg': 1.469599427784004, 'random_strength': 2.781510496712363, 'bagging_temperature': 0.7181345438925953}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:08:13,143] Trial 40 finished with value: 0.26818409034711943 and parameters: {'learning_rate': 0.17044775573679813, 'depth': 7, 'l2_leaf_reg': 2.2987276187834875, 'random_strength': 1.1909227050968312, 'bagging_temperature': 0.6843523522706263}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:10:49,190] Trial 41 finished with value: 0.26833113805180864 and parameters: {'learning_rate': 0.19891524701856567, 'depth': 6, 'l2_leaf_reg': 3.0138009149124274, 'random_strength': 0.48183864162423423, 'bagging_temperature': 0.5143172461487402}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:12:10,330] Trial 42 finished with value: 0.26696878567956045 and parameters: {'learning_rate': 0.14599653133631624, 'depth': 7, 'l2_leaf_reg': 2.069583053667563, 'random_strength': 5.505150365146644, 'bagging_temperature': 0.38410055165449586}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:13:21,454] Trial 43 finished with value: 0.2677707092139628 and parameters: {'learning_rate': 0.16843038191496112, 'depth': 5, 'l2_leaf_reg': 1.4206767626983956, 'random_strength': 3.5060196507407757, 'bagging_temperature': 0.5986489533391351}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:14:57,195] Trial 44 finished with value: 0.26828004588500787 and parameters: {'learning_rate': 0.1547676663479024, 'depth': 6, 'l2_leaf_reg': 3.709814261013736, 'random_strength': 0.5339662352353765, 'bagging_temperature': 0.47843567791902325}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:16:56,886] Trial 45 finished with value: 0.2694478194392378 and parameters: {'learning_rate': 0.1639599447702272, 'depth': 6, 'l2_leaf_reg': 4.704089142032329, 'random_strength': 2.6591603059269175, 'bagging_temperature': 0.07037581493246954}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:19:30,286] Trial 46 finished with value: 0.2667938969522679 and parameters: {'learning_rate': 0.18456162107816843, 'depth': 8, 'l2_leaf_reg': 4.891572217578252, 'random_strength': 1.8971212506767767, 'bagging_temperature': 0.08050592579950773}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:21:01,352] Trial 47 finished with value: 0.26721827791099917 and parameters: {'learning_rate': 0.15892034926671678, 'depth': 5, 'l2_leaf_reg': 7.901032292056726, 'random_strength': 2.590449191813652, 'bagging_temperature': 0.0667622815974912}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:23:02,655] Trial 48 finished with value: 0.2678453117272572 and parameters: {'learning_rate': 0.1684865136666967, 'depth': 7, 'l2_leaf_reg': 5.750549841699703, 'random_strength': 1.291673052009049, 'bagging_temperature': 0.1588635176524646}. Best is trial 30 with value: 0.2702772920654885.\n",
      "[I 2025-05-17 17:24:26,744] Trial 49 finished with value: 0.26900564062756105 and parameters: {'learning_rate': 0.1638591245556897, 'depth': 6, 'l2_leaf_reg': 2.4685797378016456, 'random_strength': 3.368222586631802, 'bagging_temperature': 0.12643005578619837}. Best is trial 30 with value: 0.2702772920654885.\n"
     ]
    }
   ],
   "source": [
    "# optimizing the hyperparameters. This may take a while...\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3eb3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('catboostranker_hyperparameters.txt', 'w') as f:\n",
    "    f.write(\"Hyperparameters:\\n\")\n",
    "    f.write(str(study.best_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "591f26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model will now be trained on the full dataset\n",
    "\n",
    "# I will set up a CatBoostRanker. It handles numeric variables natively, so only the categorical ones need to be specified (it assumes everything else is numeric).\n",
    "\n",
    "# this column is used as a sort of relevance feature for the ranking, as the most relevant hotels are the ones the customers actually booked. This will help the model decide what is most important\n",
    "train_data = train_data.to_pandas()\n",
    "train_data['target'] = train_data['booking_bool']\n",
    "\n",
    "# now we exclude target-only columns\n",
    "exclude_cols = ['target', 'booking_bool', 'srch_id', 'prop_id']\n",
    "feature_cols = [col for col in train_data.columns if col not in exclude_cols]\n",
    "\n",
    "# specifying categorical features\n",
    "cat_features = [col for col in train_data[feature_cols].columns if train_data[col].dtype == 'object' or 'id' in col]\n",
    "\n",
    "# do the train/test split by group (srch_id). This is to prevent the data being split such that both the training and test sets contain the same search id\n",
    "unique_srch_ids = train_data['srch_id'].unique()\n",
    "train_srch, val_srch = train_test_split(unique_srch_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# actually creating the training and validation sets\n",
    "train_subset = train_data[train_data['srch_id'].isin(train_srch)].copy()\n",
    "val_subset = train_data[train_data['srch_id'].isin(val_srch)].copy()\n",
    "\n",
    "train_subset = train_subset.sort_values('srch_id')\n",
    "val_subset = val_subset.sort_values('srch_id')\n",
    "\n",
    "# impute missing numeric values\n",
    "for col in feature_cols:\n",
    "    if train_subset[col].dtype in [np.float64, np.int64]:\n",
    "        train_subset[col] = train_subset[col].fillna(-999)\n",
    "    if val_subset[col].dtype in [np.float64, np.int64]:\n",
    "        val_subset[col] = val_subset[col].fillna(-999)\n",
    "\n",
    "# same with categorical\n",
    "for col in cat_features:\n",
    "    train_subset[col] = train_subset[col].fillna('missing')\n",
    "    val_subset[col] = val_subset[col].fillna('missing')\n",
    "\n",
    "# Helper to compute group sizes\n",
    "def get_group_sizes(df, group_key='srch_id'):\n",
    "    return df.groupby(group_key).size().tolist()\n",
    "\n",
    "# Group sizes for CatBoost\n",
    "train_group_sizes = get_group_sizes(train_subset)\n",
    "val_group_sizes = get_group_sizes(val_subset)\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=train_subset[feature_cols],\n",
    "    label=train_subset['target'],\n",
    "    group_id=train_subset['srch_id'].astype(int).tolist(),\n",
    "    cat_features=cat_features\n",
    ")\n",
    "\n",
    "val_pool = Pool(\n",
    "    data=val_subset[feature_cols],\n",
    "    label=val_subset['target'],\n",
    "    group_id=val_subset['srch_id'].astype(int).tolist(),\n",
    "    cat_features=cat_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "339d5155003c644d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T20:23:11.116705Z",
     "start_time": "2025-05-15T20:22:51.807070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groupwise loss function. OneHotMaxSize set to 10\n",
      "0:\ttest: 0.2506159\tbest: 0.2506159 (0)\ttotal: 4.41s\tremaining: 36m 40s\n",
      "1:\ttest: 0.2665479\tbest: 0.2665479 (1)\ttotal: 8.67s\tremaining: 35m 57s\n",
      "2:\ttest: 0.2787950\tbest: 0.2787950 (2)\ttotal: 12.4s\tremaining: 34m 11s\n",
      "3:\ttest: 0.3034428\tbest: 0.3034428 (3)\ttotal: 15.8s\tremaining: 32m 39s\n",
      "4:\ttest: 0.3129781\tbest: 0.3129781 (4)\ttotal: 19.9s\tremaining: 32m 47s\n",
      "5:\ttest: 0.3279341\tbest: 0.3279341 (5)\ttotal: 23.3s\tremaining: 32m\n",
      "6:\ttest: 0.3334153\tbest: 0.3334153 (6)\ttotal: 27.3s\tremaining: 32m 1s\n",
      "7:\ttest: 0.3346231\tbest: 0.3346231 (7)\ttotal: 30.9s\tremaining: 31m 38s\n",
      "8:\ttest: 0.3414112\tbest: 0.3414112 (8)\ttotal: 34.5s\tremaining: 31m 23s\n",
      "9:\ttest: 0.3428203\tbest: 0.3428203 (9)\ttotal: 38.5s\tremaining: 31m 27s\n",
      "10:\ttest: 0.3477656\tbest: 0.3477656 (10)\ttotal: 41.9s\tremaining: 31m 4s\n",
      "11:\ttest: 0.3479229\tbest: 0.3479229 (11)\ttotal: 45.3s\tremaining: 30m 42s\n",
      "12:\ttest: 0.3474856\tbest: 0.3479229 (11)\ttotal: 48.8s\tremaining: 30m 27s\n",
      "13:\ttest: 0.3477396\tbest: 0.3479229 (11)\ttotal: 51.8s\tremaining: 29m 57s\n",
      "14:\ttest: 0.3487659\tbest: 0.3487659 (14)\ttotal: 55.3s\tremaining: 29m 48s\n",
      "15:\ttest: 0.3485625\tbest: 0.3487659 (14)\ttotal: 59.2s\tremaining: 29m 50s\n",
      "16:\ttest: 0.3533474\tbest: 0.3533474 (16)\ttotal: 1m 3s\tremaining: 30m 2s\n",
      "17:\ttest: 0.3545082\tbest: 0.3545082 (17)\ttotal: 1m 6s\tremaining: 29m 48s\n",
      "18:\ttest: 0.3555339\tbest: 0.3555339 (18)\ttotal: 1m 10s\tremaining: 29m 36s\n",
      "19:\ttest: 0.3572014\tbest: 0.3572014 (19)\ttotal: 1m 13s\tremaining: 29m 34s\n",
      "20:\ttest: 0.3572014\tbest: 0.3572014 (19)\ttotal: 1m 16s\tremaining: 29m 13s\n",
      "21:\ttest: 0.3595402\tbest: 0.3595402 (21)\ttotal: 1m 20s\tremaining: 29m 12s\n",
      "22:\ttest: 0.3601553\tbest: 0.3601553 (22)\ttotal: 1m 24s\tremaining: 29m 4s\n",
      "23:\ttest: 0.3604393\tbest: 0.3604393 (23)\ttotal: 1m 26s\tremaining: 28m 44s\n",
      "24:\ttest: 0.3603914\tbest: 0.3604393 (23)\ttotal: 1m 30s\tremaining: 28m 47s\n",
      "25:\ttest: 0.3603914\tbest: 0.3604393 (23)\ttotal: 1m 33s\tremaining: 28m 31s\n",
      "26:\ttest: 0.3610106\tbest: 0.3610106 (26)\ttotal: 1m 37s\tremaining: 28m 26s\n",
      "27:\ttest: 0.3613233\tbest: 0.3613233 (27)\ttotal: 1m 40s\tremaining: 28m 7s\n",
      "28:\ttest: 0.3613923\tbest: 0.3613923 (28)\ttotal: 1m 42s\tremaining: 27m 50s\n",
      "29:\ttest: 0.3616201\tbest: 0.3616201 (29)\ttotal: 1m 46s\tremaining: 27m 41s\n",
      "30:\ttest: 0.3619896\tbest: 0.3619896 (30)\ttotal: 1m 49s\tremaining: 27m 30s\n",
      "31:\ttest: 0.3636842\tbest: 0.3636842 (31)\ttotal: 1m 52s\tremaining: 27m 28s\n",
      "32:\ttest: 0.3637481\tbest: 0.3637481 (32)\ttotal: 1m 55s\tremaining: 27m 14s\n",
      "33:\ttest: 0.3639103\tbest: 0.3639103 (33)\ttotal: 1m 58s\tremaining: 27m 8s\n",
      "34:\ttest: 0.3643522\tbest: 0.3643522 (34)\ttotal: 2m 2s\tremaining: 27m 5s\n",
      "35:\ttest: 0.3643595\tbest: 0.3643595 (35)\ttotal: 2m 5s\tremaining: 27m 3s\n",
      "36:\ttest: 0.3646736\tbest: 0.3646736 (36)\ttotal: 2m 9s\tremaining: 26m 56s\n",
      "37:\ttest: 0.3645961\tbest: 0.3646736 (36)\ttotal: 2m 12s\tremaining: 26m 56s\n",
      "38:\ttest: 0.3645961\tbest: 0.3646736 (36)\ttotal: 2m 15s\tremaining: 26m 46s\n",
      "39:\ttest: 0.3649789\tbest: 0.3649789 (39)\ttotal: 2m 19s\tremaining: 26m 45s\n",
      "40:\ttest: 0.3651571\tbest: 0.3651571 (40)\ttotal: 2m 22s\tremaining: 26m 36s\n",
      "41:\ttest: 0.3651571\tbest: 0.3651571 (40)\ttotal: 2m 26s\tremaining: 26m 33s\n",
      "42:\ttest: 0.3652680\tbest: 0.3652680 (42)\ttotal: 2m 29s\tremaining: 26m 25s\n",
      "43:\ttest: 0.3671417\tbest: 0.3671417 (43)\ttotal: 2m 32s\tremaining: 26m 24s\n",
      "44:\ttest: 0.3676672\tbest: 0.3676672 (44)\ttotal: 2m 35s\tremaining: 26m 16s\n",
      "45:\ttest: 0.3678194\tbest: 0.3678194 (45)\ttotal: 2m 39s\tremaining: 26m 14s\n",
      "46:\ttest: 0.3681109\tbest: 0.3681109 (46)\ttotal: 2m 42s\tremaining: 26m 9s\n",
      "47:\ttest: 0.3681109\tbest: 0.3681109 (46)\ttotal: 2m 45s\tremaining: 25m 58s\n",
      "48:\ttest: 0.3686901\tbest: 0.3686901 (48)\ttotal: 2m 48s\tremaining: 25m 49s\n",
      "49:\ttest: 0.3687997\tbest: 0.3687997 (49)\ttotal: 2m 52s\tremaining: 25m 48s\n",
      "50:\ttest: 0.3688826\tbest: 0.3688826 (50)\ttotal: 2m 54s\tremaining: 25m 40s\n",
      "51:\ttest: 0.3689557\tbest: 0.3689557 (51)\ttotal: 2m 57s\tremaining: 25m 31s\n",
      "52:\ttest: 0.3689906\tbest: 0.3689906 (52)\ttotal: 3m\tremaining: 25m 24s\n",
      "53:\ttest: 0.3692946\tbest: 0.3692946 (53)\ttotal: 3m 4s\tremaining: 25m 25s\n",
      "54:\ttest: 0.3694214\tbest: 0.3694214 (54)\ttotal: 3m 7s\tremaining: 25m 18s\n",
      "55:\ttest: 0.3693579\tbest: 0.3694214 (54)\ttotal: 3m 11s\tremaining: 25m 16s\n",
      "56:\ttest: 0.3693757\tbest: 0.3694214 (54)\ttotal: 3m 14s\tremaining: 25m 9s\n",
      "57:\ttest: 0.3695315\tbest: 0.3695315 (57)\ttotal: 3m 17s\tremaining: 25m 6s\n",
      "58:\ttest: 0.3707740\tbest: 0.3707740 (58)\ttotal: 3m 21s\tremaining: 25m 4s\n",
      "59:\ttest: 0.3709279\tbest: 0.3709279 (59)\ttotal: 3m 24s\tremaining: 25m 2s\n",
      "60:\ttest: 0.3710448\tbest: 0.3710448 (60)\ttotal: 3m 28s\tremaining: 25m 1s\n",
      "61:\ttest: 0.3710646\tbest: 0.3710646 (61)\ttotal: 3m 31s\tremaining: 24m 56s\n",
      "62:\ttest: 0.3714533\tbest: 0.3714533 (62)\ttotal: 3m 35s\tremaining: 24m 51s\n",
      "63:\ttest: 0.3714533\tbest: 0.3714533 (62)\ttotal: 3m 38s\tremaining: 24m 45s\n",
      "64:\ttest: 0.3717630\tbest: 0.3717630 (64)\ttotal: 3m 40s\tremaining: 24m 37s\n",
      "65:\ttest: 0.3716692\tbest: 0.3717630 (64)\ttotal: 3m 43s\tremaining: 24m 27s\n",
      "66:\ttest: 0.3717545\tbest: 0.3717630 (64)\ttotal: 3m 46s\tremaining: 24m 25s\n",
      "67:\ttest: 0.3724477\tbest: 0.3724477 (67)\ttotal: 3m 50s\tremaining: 24m 23s\n",
      "68:\ttest: 0.3726623\tbest: 0.3726623 (68)\ttotal: 3m 54s\tremaining: 24m 22s\n",
      "69:\ttest: 0.3726623\tbest: 0.3726623 (68)\ttotal: 3m 56s\tremaining: 24m 13s\n",
      "70:\ttest: 0.3729639\tbest: 0.3729639 (70)\ttotal: 4m\tremaining: 24m 11s\n",
      "71:\ttest: 0.3730613\tbest: 0.3730613 (71)\ttotal: 4m 3s\tremaining: 24m 10s\n",
      "72:\ttest: 0.3733440\tbest: 0.3733440 (72)\ttotal: 4m 8s\tremaining: 24m 10s\n",
      "73:\ttest: 0.3733636\tbest: 0.3733636 (73)\ttotal: 4m 11s\tremaining: 24m 10s\n",
      "74:\ttest: 0.3734712\tbest: 0.3734712 (74)\ttotal: 4m 15s\tremaining: 24m 8s\n",
      "75:\ttest: 0.3734632\tbest: 0.3734712 (74)\ttotal: 4m 18s\tremaining: 24m 1s\n",
      "76:\ttest: 0.3736248\tbest: 0.3736248 (76)\ttotal: 4m 21s\tremaining: 23m 57s\n",
      "77:\ttest: 0.3737644\tbest: 0.3737644 (77)\ttotal: 4m 25s\tremaining: 23m 58s\n",
      "78:\ttest: 0.3739160\tbest: 0.3739160 (78)\ttotal: 4m 29s\tremaining: 23m 56s\n",
      "79:\ttest: 0.3742990\tbest: 0.3742990 (79)\ttotal: 4m 32s\tremaining: 23m 51s\n",
      "80:\ttest: 0.3743877\tbest: 0.3743877 (80)\ttotal: 4m 37s\tremaining: 23m 52s\n",
      "81:\ttest: 0.3743349\tbest: 0.3743877 (80)\ttotal: 4m 41s\tremaining: 23m 53s\n",
      "82:\ttest: 0.3743216\tbest: 0.3743877 (80)\ttotal: 4m 45s\tremaining: 23m 52s\n",
      "83:\ttest: 0.3744392\tbest: 0.3744392 (83)\ttotal: 4m 48s\tremaining: 23m 50s\n",
      "84:\ttest: 0.3745186\tbest: 0.3745186 (84)\ttotal: 4m 52s\tremaining: 23m 50s\n",
      "85:\ttest: 0.3746163\tbest: 0.3746163 (85)\ttotal: 4m 57s\tremaining: 23m 50s\n",
      "86:\ttest: 0.3745761\tbest: 0.3746163 (85)\ttotal: 5m\tremaining: 23m 46s\n",
      "87:\ttest: 0.3746947\tbest: 0.3746947 (87)\ttotal: 5m 3s\tremaining: 23m 41s\n",
      "88:\ttest: 0.3746947\tbest: 0.3746947 (87)\ttotal: 5m 6s\tremaining: 23m 35s\n",
      "89:\ttest: 0.3748479\tbest: 0.3748479 (89)\ttotal: 5m 10s\tremaining: 23m 33s\n",
      "90:\ttest: 0.3749848\tbest: 0.3749848 (90)\ttotal: 5m 14s\tremaining: 23m 32s\n",
      "91:\ttest: 0.3749848\tbest: 0.3749848 (90)\ttotal: 5m 17s\tremaining: 23m 29s\n",
      "92:\ttest: 0.3749956\tbest: 0.3749956 (92)\ttotal: 5m 21s\tremaining: 23m 25s\n",
      "93:\ttest: 0.3752150\tbest: 0.3752150 (93)\ttotal: 5m 24s\tremaining: 23m 23s\n",
      "94:\ttest: 0.3752150\tbest: 0.3752150 (93)\ttotal: 5m 27s\tremaining: 23m 15s\n",
      "95:\ttest: 0.3752150\tbest: 0.3752150 (93)\ttotal: 5m 30s\tremaining: 23m 9s\n",
      "96:\ttest: 0.3752220\tbest: 0.3752220 (96)\ttotal: 5m 33s\tremaining: 23m 6s\n",
      "97:\ttest: 0.3752474\tbest: 0.3752474 (97)\ttotal: 5m 36s\tremaining: 23m\n",
      "98:\ttest: 0.3752075\tbest: 0.3752474 (97)\ttotal: 5m 39s\tremaining: 22m 54s\n",
      "99:\ttest: 0.3751858\tbest: 0.3752474 (97)\ttotal: 5m 42s\tremaining: 22m 50s\n",
      "100:\ttest: 0.3751858\tbest: 0.3752474 (97)\ttotal: 5m 45s\tremaining: 22m 43s\n",
      "101:\ttest: 0.3752404\tbest: 0.3752474 (97)\ttotal: 5m 48s\tremaining: 22m 41s\n",
      "102:\ttest: 0.3753354\tbest: 0.3753354 (102)\ttotal: 5m 53s\tremaining: 22m 40s\n",
      "103:\ttest: 0.3753356\tbest: 0.3753356 (103)\ttotal: 5m 56s\tremaining: 22m 37s\n",
      "104:\ttest: 0.3753488\tbest: 0.3753488 (104)\ttotal: 5m 59s\tremaining: 22m 31s\n",
      "105:\ttest: 0.3753760\tbest: 0.3753760 (105)\ttotal: 6m 3s\tremaining: 22m 29s\n",
      "106:\ttest: 0.3752835\tbest: 0.3753760 (105)\ttotal: 6m 6s\tremaining: 22m 26s\n",
      "107:\ttest: 0.3752497\tbest: 0.3753760 (105)\ttotal: 6m 10s\tremaining: 22m 26s\n",
      "108:\ttest: 0.3752824\tbest: 0.3753760 (105)\ttotal: 6m 14s\tremaining: 22m 22s\n",
      "109:\ttest: 0.3753025\tbest: 0.3753760 (105)\ttotal: 6m 17s\tremaining: 22m 18s\n",
      "110:\ttest: 0.3752523\tbest: 0.3753760 (105)\ttotal: 6m 21s\tremaining: 22m 15s\n",
      "111:\ttest: 0.3753089\tbest: 0.3753760 (105)\ttotal: 6m 24s\tremaining: 22m 12s\n",
      "112:\ttest: 0.3753276\tbest: 0.3753760 (105)\ttotal: 6m 28s\tremaining: 22m 9s\n",
      "113:\ttest: 0.3755061\tbest: 0.3755061 (113)\ttotal: 6m 31s\tremaining: 22m 5s\n",
      "114:\ttest: 0.3757100\tbest: 0.3757100 (114)\ttotal: 6m 34s\tremaining: 22m\n",
      "115:\ttest: 0.3757866\tbest: 0.3757866 (115)\ttotal: 6m 37s\tremaining: 21m 54s\n",
      "116:\ttest: 0.3757298\tbest: 0.3757866 (115)\ttotal: 6m 40s\tremaining: 21m 52s\n",
      "117:\ttest: 0.3758106\tbest: 0.3758106 (117)\ttotal: 6m 44s\tremaining: 21m 50s\n",
      "118:\ttest: 0.3759244\tbest: 0.3759244 (118)\ttotal: 6m 48s\tremaining: 21m 47s\n",
      "119:\ttest: 0.3759550\tbest: 0.3759550 (119)\ttotal: 6m 51s\tremaining: 21m 42s\n",
      "120:\ttest: 0.3760378\tbest: 0.3760378 (120)\ttotal: 6m 54s\tremaining: 21m 38s\n",
      "121:\ttest: 0.3760122\tbest: 0.3760378 (120)\ttotal: 6m 57s\tremaining: 21m 34s\n",
      "122:\ttest: 0.3757521\tbest: 0.3760378 (120)\ttotal: 7m 1s\tremaining: 21m 32s\n",
      "123:\ttest: 0.3758345\tbest: 0.3760378 (120)\ttotal: 7m 5s\tremaining: 21m 29s\n",
      "124:\ttest: 0.3757368\tbest: 0.3760378 (120)\ttotal: 7m 9s\tremaining: 21m 27s\n",
      "125:\ttest: 0.3757379\tbest: 0.3760378 (120)\ttotal: 7m 12s\tremaining: 21m 23s\n",
      "126:\ttest: 0.3761156\tbest: 0.3761156 (126)\ttotal: 7m 15s\tremaining: 21m 19s\n",
      "127:\ttest: 0.3761827\tbest: 0.3761827 (127)\ttotal: 7m 19s\tremaining: 21m 16s\n",
      "128:\ttest: 0.3762276\tbest: 0.3762276 (128)\ttotal: 7m 22s\tremaining: 21m 12s\n",
      "129:\ttest: 0.3764348\tbest: 0.3764348 (129)\ttotal: 7m 26s\tremaining: 21m 11s\n",
      "130:\ttest: 0.3764665\tbest: 0.3764665 (130)\ttotal: 7m 30s\tremaining: 21m 8s\n",
      "131:\ttest: 0.3764892\tbest: 0.3764892 (131)\ttotal: 7m 34s\tremaining: 21m 6s\n",
      "132:\ttest: 0.3764454\tbest: 0.3764892 (131)\ttotal: 7m 38s\tremaining: 21m 6s\n",
      "133:\ttest: 0.3767750\tbest: 0.3767750 (133)\ttotal: 7m 41s\tremaining: 21m 1s\n",
      "134:\ttest: 0.3767969\tbest: 0.3767969 (134)\ttotal: 7m 45s\tremaining: 20m 58s\n",
      "135:\ttest: 0.3768691\tbest: 0.3768691 (135)\ttotal: 7m 49s\tremaining: 20m 56s\n",
      "136:\ttest: 0.3767623\tbest: 0.3768691 (135)\ttotal: 7m 53s\tremaining: 20m 53s\n",
      "137:\ttest: 0.3767544\tbest: 0.3768691 (135)\ttotal: 7m 57s\tremaining: 20m 52s\n",
      "138:\ttest: 0.3768427\tbest: 0.3768691 (135)\ttotal: 8m\tremaining: 20m 49s\n",
      "139:\ttest: 0.3769074\tbest: 0.3769074 (139)\ttotal: 8m 4s\tremaining: 20m 46s\n",
      "140:\ttest: 0.3768261\tbest: 0.3769074 (139)\ttotal: 8m 8s\tremaining: 20m 44s\n",
      "141:\ttest: 0.3768295\tbest: 0.3769074 (139)\ttotal: 8m 12s\tremaining: 20m 42s\n",
      "142:\ttest: 0.3768425\tbest: 0.3769074 (139)\ttotal: 8m 16s\tremaining: 20m 38s\n",
      "143:\ttest: 0.3768090\tbest: 0.3769074 (139)\ttotal: 8m 19s\tremaining: 20m 35s\n",
      "144:\ttest: 0.3766851\tbest: 0.3769074 (139)\ttotal: 8m 23s\tremaining: 20m 33s\n",
      "145:\ttest: 0.3766942\tbest: 0.3769074 (139)\ttotal: 8m 27s\tremaining: 20m 29s\n",
      "146:\ttest: 0.3767131\tbest: 0.3769074 (139)\ttotal: 8m 30s\tremaining: 20m 26s\n",
      "147:\ttest: 0.3767656\tbest: 0.3769074 (139)\ttotal: 8m 34s\tremaining: 20m 23s\n",
      "148:\ttest: 0.3767949\tbest: 0.3769074 (139)\ttotal: 8m 37s\tremaining: 20m 19s\n",
      "149:\ttest: 0.3768221\tbest: 0.3769074 (139)\ttotal: 8m 41s\tremaining: 20m 17s\n",
      "150:\ttest: 0.3768066\tbest: 0.3769074 (139)\ttotal: 8m 45s\tremaining: 20m 14s\n",
      "151:\ttest: 0.3768428\tbest: 0.3769074 (139)\ttotal: 8m 49s\tremaining: 20m 11s\n",
      "152:\ttest: 0.3768428\tbest: 0.3769074 (139)\ttotal: 8m 52s\tremaining: 20m 8s\n",
      "153:\ttest: 0.3767469\tbest: 0.3769074 (139)\ttotal: 8m 56s\tremaining: 20m 4s\n",
      "154:\ttest: 0.3766457\tbest: 0.3769074 (139)\ttotal: 8m 59s\tremaining: 20m\n",
      "155:\ttest: 0.3765575\tbest: 0.3769074 (139)\ttotal: 9m 2s\tremaining: 19m 57s\n",
      "156:\ttest: 0.3766177\tbest: 0.3769074 (139)\ttotal: 9m 6s\tremaining: 19m 54s\n",
      "157:\ttest: 0.3767011\tbest: 0.3769074 (139)\ttotal: 9m 10s\tremaining: 19m 51s\n",
      "158:\ttest: 0.3766665\tbest: 0.3769074 (139)\ttotal: 9m 14s\tremaining: 19m 48s\n",
      "159:\ttest: 0.3766480\tbest: 0.3769074 (139)\ttotal: 9m 18s\tremaining: 19m 46s\n",
      "160:\ttest: 0.3766840\tbest: 0.3769074 (139)\ttotal: 9m 22s\tremaining: 19m 43s\n",
      "161:\ttest: 0.3766944\tbest: 0.3769074 (139)\ttotal: 9m 25s\tremaining: 19m 40s\n",
      "162:\ttest: 0.3767181\tbest: 0.3769074 (139)\ttotal: 9m 29s\tremaining: 19m 37s\n",
      "163:\ttest: 0.3768666\tbest: 0.3769074 (139)\ttotal: 9m 33s\tremaining: 19m 34s\n",
      "164:\ttest: 0.3768803\tbest: 0.3769074 (139)\ttotal: 9m 37s\tremaining: 19m 31s\n",
      "165:\ttest: 0.3768295\tbest: 0.3769074 (139)\ttotal: 9m 41s\tremaining: 19m 29s\n",
      "166:\ttest: 0.3771105\tbest: 0.3771105 (166)\ttotal: 9m 44s\tremaining: 19m 26s\n",
      "167:\ttest: 0.3770741\tbest: 0.3771105 (166)\ttotal: 9m 48s\tremaining: 19m 23s\n",
      "168:\ttest: 0.3770523\tbest: 0.3771105 (166)\ttotal: 9m 52s\tremaining: 19m 20s\n",
      "169:\ttest: 0.3769837\tbest: 0.3771105 (166)\ttotal: 9m 56s\tremaining: 19m 17s\n",
      "170:\ttest: 0.3769906\tbest: 0.3771105 (166)\ttotal: 10m\tremaining: 19m 15s\n",
      "171:\ttest: 0.3770338\tbest: 0.3771105 (166)\ttotal: 10m 4s\tremaining: 19m 12s\n",
      "172:\ttest: 0.3770130\tbest: 0.3771105 (166)\ttotal: 10m 8s\tremaining: 19m 10s\n",
      "173:\ttest: 0.3769700\tbest: 0.3771105 (166)\ttotal: 10m 12s\tremaining: 19m 6s\n",
      "174:\ttest: 0.3769878\tbest: 0.3771105 (166)\ttotal: 10m 15s\tremaining: 19m 3s\n",
      "175:\ttest: 0.3770088\tbest: 0.3771105 (166)\ttotal: 10m 19s\tremaining: 19m\n",
      "176:\ttest: 0.3770308\tbest: 0.3771105 (166)\ttotal: 10m 22s\tremaining: 18m 56s\n",
      "177:\ttest: 0.3770662\tbest: 0.3771105 (166)\ttotal: 10m 26s\tremaining: 18m 53s\n",
      "178:\ttest: 0.3769371\tbest: 0.3771105 (166)\ttotal: 10m 30s\tremaining: 18m 50s\n",
      "179:\ttest: 0.3768690\tbest: 0.3771105 (166)\ttotal: 10m 34s\tremaining: 18m 47s\n",
      "180:\ttest: 0.3768423\tbest: 0.3771105 (166)\ttotal: 10m 37s\tremaining: 18m 44s\n",
      "181:\ttest: 0.3768597\tbest: 0.3771105 (166)\ttotal: 10m 41s\tremaining: 18m 40s\n",
      "182:\ttest: 0.3768017\tbest: 0.3771105 (166)\ttotal: 10m 45s\tremaining: 18m 37s\n",
      "183:\ttest: 0.3768272\tbest: 0.3771105 (166)\ttotal: 10m 49s\tremaining: 18m 34s\n",
      "184:\ttest: 0.3768528\tbest: 0.3771105 (166)\ttotal: 10m 52s\tremaining: 18m 31s\n",
      "185:\ttest: 0.3768528\tbest: 0.3771105 (166)\ttotal: 10m 56s\tremaining: 18m 27s\n",
      "186:\ttest: 0.3768405\tbest: 0.3771105 (166)\ttotal: 11m\tremaining: 18m 25s\n",
      "187:\ttest: 0.3768966\tbest: 0.3771105 (166)\ttotal: 11m 4s\tremaining: 18m 22s\n",
      "188:\ttest: 0.3769221\tbest: 0.3771105 (166)\ttotal: 11m 7s\tremaining: 18m 19s\n",
      "189:\ttest: 0.3770105\tbest: 0.3771105 (166)\ttotal: 11m 11s\tremaining: 18m 16s\n",
      "190:\ttest: 0.3769837\tbest: 0.3771105 (166)\ttotal: 11m 15s\tremaining: 18m 12s\n",
      "191:\ttest: 0.3769462\tbest: 0.3771105 (166)\ttotal: 11m 19s\tremaining: 18m 9s\n",
      "192:\ttest: 0.3770048\tbest: 0.3771105 (166)\ttotal: 11m 23s\tremaining: 18m 6s\n",
      "193:\ttest: 0.3770048\tbest: 0.3771105 (166)\ttotal: 11m 26s\tremaining: 18m 2s\n",
      "194:\ttest: 0.3769742\tbest: 0.3771105 (166)\ttotal: 11m 29s\tremaining: 17m 58s\n",
      "195:\ttest: 0.3769820\tbest: 0.3771105 (166)\ttotal: 11m 33s\tremaining: 17m 55s\n",
      "196:\ttest: 0.3770324\tbest: 0.3771105 (166)\ttotal: 11m 36s\tremaining: 17m 51s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 0.3771105124\n",
      "bestIteration = 166\n",
      "\n",
      "Shrink model to first 167 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRanker at 0x1a2fcc240b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying the best hyperparameters to the model and training it\n",
    "best_params = study.best_params\n",
    "best_params[\"iterations\"] = 500\n",
    "best_params[\"loss_function\"] = \"YetiRank\"\n",
    "\n",
    "model = CatBoostRanker(**best_params)\n",
    "model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd49f58d82dc4caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T20:27:57.132275Z",
     "start_time": "2025-05-15T20:27:56.165160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         srch_id  prop_id\n",
      "23             1    99484\n",
      "9              1    54937\n",
      "12             1    61934\n",
      "5              1    28181\n",
      "4              1    24194\n",
      "...          ...      ...\n",
      "4959179   332787    33959\n",
      "4959178   332787    32019\n",
      "4959182   332787    99509\n",
      "4959181   332787    94437\n",
      "4959180   332787    35240\n",
      "\n",
      "[4959183 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test_subset = test_data.to_pandas().copy()\n",
    "\n",
    "# impute missing numeric values\n",
    "for col in feature_cols:\n",
    "    if test_subset[col].dtype in [np.float64, np.int64]:\n",
    "        test_subset[col] = test_subset[col].fillna(-999)\n",
    "\n",
    "# same with categorical\n",
    "for col in cat_features:\n",
    "    test_subset[col] = test_subset[col].fillna('missing')\n",
    "\n",
    "# Apply model on test data\n",
    "test_pool = Pool(test_subset[feature_cols], group_id=test_subset['srch_id'], cat_features=cat_features)\n",
    "test_subset['pred_score'] = model.predict(test_pool)\n",
    "\n",
    "# Rank hotels for each search\n",
    "ranked = test_subset.sort_values(by=['srch_id', 'pred_score'], ascending=[True, False])\n",
    "\n",
    "# the prediction score from catboost isnt a probability, it instead is a relevance score. Converting this to a probability\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# apply softmax to the prediction scores\n",
    "ranked['softmax_score'] = ranked.groupby('srch_id')['pred_score'].transform(softmax)\n",
    "ranked = ranked[['srch_id', 'prop_id']]\n",
    "print(ranked)\n",
    "\n",
    "# save the ranked dataframe to a csv\n",
    "ranked.to_csv('catboostranker_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb71879c7a9ceb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
